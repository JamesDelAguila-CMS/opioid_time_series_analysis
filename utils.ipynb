{"cells":[{"cell_type":"code","source":["from pyspark.sql.functions import broadcast, when, sequence, to_date, explode, col, expr, struct, collect_list, max, udf, sort_array, date_add, to_date, udf\nfrom pyspark.sql import DataFrame, Window\nfrom pyspark.sql.types import TimestampType\nimport numpy as np\n\nclass preprocess_timeseries:\n  '''\n  \n  Author: James DelAguila (CMS)\n  \n  Certain time-series analysis libraries like sktime require table formats such as nested array, where each element of the array consists of 1 period (e.g., 1 day). This class processes a Spark data table composed of line-level claims data and provides a Spark dataframe with nested array 'panel' layout where each element of the array consists of 1 day. This Spark table layout can be converted to a pandas dataframe via the to_pd() function, or used as a standalone.\n  \n  Note that the execution time and ability to convert to pandas dataframe depend heavily on spark configuration. For pandas conversion, these settings worked for our use-case of ~600k individuals, 24 time series, with 305 days in each array: spark.driver.memory = 40g, maxresultsize = 20g, network.timeout = 10000000, at least 2 workers online.\n  \n  Parameters:\n  ------------\n  \n  cohort_df: Spark dataframe, a dataframe consisting of relevant claims data from Medicare claims analytical tables\n  target_col_name: Name of target column from cohort_df\n  date_col_name: str, the name of the column in the cohort dataframe that drives the time-series designation (eg., 'frst_srvc_dt' in ELDB)\n  sample: float, Proportional random sample of beneficiaries to request full claims history. Full sample if unspecified\n  randomseed: int, random seed to apply to sample. 42 if unspecified\n  \n  \n  Example Usages:\n  ______________\n  \n  # For a Spark table\n  df = (preprocess_timeseries({cohort_df}, sample=1.0, random_seed = 42)\n          .build_time_series(date_col_name = 'first_srvc_dt',\n                      start_date = '2020-01-01', \n                      end_date = '2020-10-31',\n                      flag_dict = flag_dict,\n                      broadcast_dict = broadcast_dict)\n          .convert_ts_2_array(flag_dict = flag_dict,\n                        broadcast_dict = broadcast_dict)\n          .output()\n         )\n   \n   # For a Pandas table      \n   df = (preprocess_timeseries({cohort_df}, sample=1.0, random_seed = 42)\n          .build_time_series(date_col_name = 'first_srvc_dt',\n                      start_date = '2020-01-01', \n                      end_date = '2020-10-31',\n                      flag_dict = flag_dict,\n                      broadcast_dict = broadcast_dict)\n          .convert_ts_2_array(flag_dict = flag_dict,\n                        broadcast_dict = broadcast_dict)\n          .to_pd()\n          .output()\n         )\n  '''\n  \n  def __init__(self, cohort_df, target_col_name = 'target', sample=1.0, random_seed=42):\n    self.sample = float(sample)\n    self.random_seed = random_seed\n    self.target_col_name = target_col_name\n    self.cohort_df = cohort_df\n    self.cohort_df = self.get_random_benes()\n   \n  def get_random_benes(self):\n    '''\n    Returns sampled dataframe to preprocess_timeseries class\n    \n    '''\n    # Select a sample of distinct beneficiaries and audit\n    cohort_df = self.cohort_df\n    sample = self.sample \n    random_seed = self.random_seed\n    distinct_benes = cohort_df.select('bene_id').distinct().orderBy('bene_id').sample(fraction=sample, seed=random_seed).cache()\n    cohort_df = cohort_df.join(distinct_benes, 'bene_id', 'inner')\n    print('Created bene sample and cached dataframe with distinct benes: ', distinct_benes.count())\n        \n    return cohort_df\n  \n  def build_time_series(self, date_col_name, start_date, end_date, flag_dict, flag_list_max, broadcast_dict, cohort_df=None):\n  \n    '''\n    For start date to end date, builds a table that contains 1 day/1 bene indicator for each condition set out in the dictionaries.\n\n    Parameters:\n    ------------\n\n    start_date: start date of time-series test period (YYYY-MM-DD)\n    end_date: start date of time-series test period (YYYY-MM-DD) \n    flag_dict: Dictionary containing pairs indicating naming of condition for which a time-series is constructed and Spark SQL conditions. \n    broadast_dict: Dictionary containing pairs indicating naming of condition for which a time-series is constructed and Spark SQL conditions. Broadcasts result to all days from Rx fill date to Rx fill date + days_supply\n    \n    '''\n    cohort_df = self.cohort_df\n    \n    cohort_df = cohort_df.withColumnRenamed(date_col_name, 'date')\n    date_df = spark.sql(f\"SELECT sequence(to_date('{start_date}'), to_date('{end_date}'), interval 1 day) as date\").withColumn(\"date\", explode(col(\"date\")))\n\n    # Path for features that do not broadcast to day\n    noa_cohort_df = (broadcast(date_df.select(\"date\").distinct())\n    .crossJoin(cohort_df.select(\"bene_id\").distinct())\n    .join(cohort_df, ['date', 'bene_id'], \"leftouter\")\n    ).orderBy('bene_id', 'date')\n\n    # Path for features that broadcast to day\n    opioid_cohort_df = (broadcast(date_df.select(\"date\").distinct())\n    .crossJoin(cohort_df.select(\"bene_id\").distinct()).alias('a')\n    .join(cohort_df.filter(\"pymt_sys = 'PTD-DRUG'\").alias('b'), expr(\"a.bene_id = b.bene_id AND a.date between b.date and date_add(b.date, cast(b.clm_days as int))\"), \"leftouter\")\n    ).orderBy('a.bene_id', 'a.date')\n\n    # Process non-opioid agonist conditions and add column   \n    for key, value in flag_dict.items():\n      noa_cohort_df = noa_cohort_df.withColumn(key, expr(value))\n\n    # Process opioid agonist conditions and add columns\n    for key, value in broadcast_dict.items():\n      opioid_cohort_df = opioid_cohort_df.withColumn(key, expr(value)).filter(\"pymt_sys = 'PTD-DRUG'\")\n\n    # Get sum of rows for time series\n    noa_list = list(flag_dict.keys())\n    noa_list.extend(['bene_id','date'])\n\n    opx_list = list(broadcast_dict.keys())\n    opx_list.extend(['a.bene_id', 'a.date'])\n\n    agg_dict = {}\n    sum_keys = [c for c in noa_list if c not in flag_list_max]\n    sum_keys.remove('bene_id')\n    sum_keys.remove('date')\n    max_keys = flag_list_max\n\n    for i in sum_keys:\n        agg_dict[i] = 'sum'\n    for i in max_keys:\n        agg_dict[i] = 'max'\n    \n    noa_cohort_df = (noa_cohort_df.select(noa_list)\n                     .groupBy('bene_id', 'date')\n                     .agg(agg_dict)\n                    )\n    \n    opioid_cohort_df = (opioid_cohort_df.select(opx_list)\n                          .groupBy('bene_id', 'date')\n                          .sum()\n                       )\n\n    # Rename fields back to assigned values\n    for key in flag_dict.keys():\n      noa_cohort_df = (noa_cohort_df.withColumnRenamed('sum(' + str(key) + ')', key)\n                                    .withColumnRenamed('max(' + str(key) + ')', key)\n                      )\n    for key in broadcast_dict.keys():\n      opioid_cohort_df = opioid_cohort_df.withColumnRenamed('sum(' + str(key) + ')', key)  \n\n    # Join back broadcasting/non-broadcasting dataframes\n    cohort_df = (noa_cohort_df.join(opioid_cohort_df, ['date', 'bene_id'], \"leftouter\")\n                 .na.fill(0).orderBy('bene_id', 'date').cache()\n                )\n    \n    print('Built long time-series with total rows: ', cohort_df.count(), ' and ', cohort_df.select('bene_id').distinct().count(), ' benes') \n                 # If you don't the cache AND this, you get different random benes every iteration (lazy eval)\n    self.cohort_df = cohort_df\n    return self\n\n  def convert_ts_2_array(self, flag_dict, broadcast_dict):\n    '''\n    Builds a Spark Dataframe with time series for each measure in dictionaries as array within cell for each beneficiary - panel layout within a Spark Dataframe\n    \n    Parameters:\n    ------------\n  \n    flag_dict: Dictionary containing pairs indicating naming of condition for which a time-series is constructed and Spark SQL conditions. \n    broadast_dict: Dictionary containing pairs indicating naming of condition for which a time-series is constructed and Spark SQL conditions. Broadcasts result to all days from Rx fill date to Rx fill date + days_supply\n    '''\n\n    cohort_df = self.cohort_df\n    # Build array timelines in cell\n    ndf2_schema = 0\n    merged_dictionaries = {**flag_dict, **broadcast_dict}\n    for key in merged_dictionaries.keys():\n\n      jump_df = (cohort_df\n                 .groupBy(\"bene_id\")\n                 .agg(sort_array(collect_list(struct('date', key)))  # Ensures proper ordering of array list\n                 .alias('collected_list'))\n                 .withColumn(key, col(f\"collected_list.{key}\"))\n                 #.withColumn(key, col(f\"collected_list\")) # for testing\n                 .drop(\"collected_list\")\n                ).orderBy('bene_id')\n\n      jump_df = jump_df.withColumnRenamed(jump_df.columns[-1], key)\n\n      # For the first key\n      if ndf2_schema < 1:\n        ndf2=jump_df\n      # All others\n      else:\n        ndf2 = ndf2.join(jump_df, 'bene_id', 'outer')\n\n      ndf2_schema +=1\n    \n    cohort_df = ndf2\n    self.cohort_df = cohort_df\n    return self\n  \n  def to_pd(self):\n    from sktime.datatypes._panel._convert import is_nested_dataframe\n    import pandas as pd\n    '''\n    Converts Spark nested panel layout to Pandas dataframe compatible with sktime libary. Note that this process may be very memory intensive - if it fails, considering increasing spark settings: sparkdriver.memory, maxresultsize, network.timeout\n    '''\n    cohort_df = self.cohort_df\n    # set arrow\n    spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n    spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n    \n    partitions = int(round((cohort_df.count()/5) + 10000, -4))\n    print('Converting to pandas...')\n    # Crazy high partition size helps this not crash\n    cohort_df = cohort_df.repartition(partitions).toPandas()\n\n    # Turn each element of columns into nested pandas series, as opposed to np.ndarray\n    cohort_df[[col for col in cohort_df.columns if isinstance(cohort_df[col], np.ndarray)]] = cohort_df[[col for col in cohort_df.columns if isinstance(cohort_df[col], np.ndarray)]].applymap(lambda x: pd.Series(x))\n    \n    self.cohort_df = cohort_df\n    \n    return self  \n    \n  def output(self):\n    '''Output dataframe at any stage before convert to pandas'''\n    cohort_df = self.cohort_df\n    return cohort_df"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"695798da-ed93-448c-b355-adcef2b14ac6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b899f482-37de-4fe4-a6cb-1ff73ebb704f"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"utils","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":361240}},"nbformat":4,"nbformat_minor":0}
